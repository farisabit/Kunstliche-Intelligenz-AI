{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren eines Classifizierers auf dem STL-10 Datensatz\n",
    "Der STL-10 Datensatz enthält 10 Klassen. Die Klassifizierer arbeiten jedoch selten mit der textuellen Beschreibung einer Klasse wie z.B. Hund, Katze, Vogel, stattdessen wird jede Klasse mit einer Zahl repräsentiert. Diese Repräsentierung legen wir als erstes fest, da uns dies später einiges erleichtern wird. Wir wählen die lexikographische Sortierung der Klassennamen und indizieren gemäß dieser Reihenfolge. Hierfür verwenden wir ein Dictionary, eine Liste wäre allerdings auch ausreichend.\n",
    "\n",
    "## Definieren einiger Konstanten\n",
    "Als nächstes definieren wir den Pfad zu den Bildern *dataset_path* und den Pfad, an dem wir später unsere trainierten Gewichte inkl. Optimiererdaten und Trainingsergebnisse speichern werden *checkpoint_dir*. In dem Verzeichnis, in dem Sie dieses Notebook speichern, wird ein Ordner angelegt mit dem Namen *Checkpoints*. Überprüfen Sie diesen von Zeit zu Zeit und löschen Sie nicht benötigte Checkpoints. Anschließend definieren wir zwei Pfade inkl. Dateinamen für diese Checkpoints. Ein Checkpoint enthält einen Zeitstempel im Dateinamen, der andere nicht. Wenn Sie später ihr Modell testen oder nochmal trainieren möchten, wird immer der Checkpoint ohne Zeitstempel zum Laden verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "# Setze einen Seed für die Reproduzierbarkeit.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class_map_idx_to_str = {0: 'airplane',\n",
    "                        1: 'bird', \n",
    "                        2: 'car', \n",
    "                        3: 'cat', \n",
    "                        4: 'deer', \n",
    "                        5: 'dog', \n",
    "                        6: 'horse', \n",
    "                        7: 'monkey', \n",
    "                        8: 'ship', \n",
    "                        9: 'truck'}\n",
    "\n",
    "\n",
    "# Definiere den Pfad zu den Ordnern train, val und test.\n",
    "dataset_path = \"/home/share/STL10/\"\n",
    "\n",
    "\n",
    "\n",
    "# Definiere den Pfad zu den trainierten Gewichten.\n",
    "checkpoint_dir = os.getcwd() + os.sep + \"Checkpoints\" + os.sep\n",
    "\n",
    "# Auslesen der aktuellen Zeit. Wenn ein neuer Checkpoint erstellt werden soll, muss diese Zelle ausgeführt werden.\n",
    "date_time = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H_%M_%S')\n",
    "\n",
    "checkpoint_save_path_1 = os.getcwd() + \"/Checkpoints/checkpoint_[\" + date_time + \"].pth.tar\"\n",
    "checkpoint_save_path_2 = os.getcwd() + \"/Checkpoints/checkpoint.pth.tar\"\n",
    "checkpoint_load_path = checkpoint_save_path_2\n",
    "\n",
    "# Erstelle einen neuen Ordner Checkpoints im aktuellen Verzeichnis.\n",
    "if not os.path.isdir(os.getcwd() + \"/Checkpoints/\"):\n",
    "    os.makedirs(os.getcwd() + \"/Checkpoints/\")\n",
    "\n",
    "# Lade den aktuellen Checkpoint (falls vorhanden), ansonsten erstelle einen neuen.\n",
    "def create_or_load_checkpoint(model, epoch=-1, optimizer=None, last_loss=10**20):\n",
    "    if os.path.isfile(checkpoint_load_path):\n",
    "        print(\"=> Looking for checkpoint\")\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_load_path)\n",
    "            epoch = checkpoint['epoch']\n",
    "            print(\"Loaded epoch number is\", epoch)            \n",
    "            last_loss = checkpoint['best_loss']\n",
    "            print(\"Loaded best loss is\", last_loss)\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            if not optimizer is None:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> Found and loaded checkpoint\")\n",
    "        except:\n",
    "            print(\"Your checkpoint does not contain trained weights. Your old weights will be overwritten.\")\n",
    "            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'best_loss': last_loss, 'optimizer': optimizer.state_dict()}, checkpoint_save_path_1)\n",
    "            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'best_loss': last_loss, 'optimizer': optimizer.state_dict()}, checkpoint_save_path_2)\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'best_loss': last_loss, 'optimizer': optimizer.state_dict()}, checkpoint_save_path_1)\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'best_loss': last_loss, 'optimizer': optimizer.state_dict()}, checkpoint_save_path_2)\n",
    "        print(\"=> No checkpoint found. You have to train first.\")\n",
    "    return epoch, last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bilder laden und Exkurs zu Augmentierungen\n",
    "Im folgenden Abschnitt laden wir die Bilder. Für größere Projekte definiert man in der Regel noch Augmentierungen, also Funktionen, die die originalen Daten ändern. Hier eine Liste gängiger Augmentierungen:\n",
    "1. Änderung in Helligkeit, Sättigung und Farbwert: verwende HSV statt RGB und ändere H (Hue), S (Saturation) und V (Value)\n",
    "2. Rotierung: Bild wird um paar Grad gedreht\n",
    "3. Verschiebung: Bild wird etwas nach oben, unten, links oder rechts verschoben.\n",
    "4. Skalierung: Bild wird vergrößert oder verkleinert\n",
    "5. Scherung: Bild wird geschert: Eine Seite bleibt fest, die gegenüberliegende Seite wird verschoben\n",
    "6. Random-Flip: Bild wird horizontal oder vertikal gespiegelt\n",
    "7. Crops: Statt dem gesamten Bild werden dem Modell nur Teile des Bildes gezeigt (meist zufällige Bildausschnitte und mehrere)\n",
    "8. Rauschen: Auf das Bild wird ein verrauschtes Array addiert\n",
    "\n",
    "Die Idee der Augmentierung ist, dass man künstlich die Anzahl der Eingabebilder vergrößert. Mittlerweile ist man so weit, dass man neuronale Netze verwendet, sogenannte GANs (Generative Adversarial Network), die für einen gegebenen Datensatz, die Verteilung dieses Datensatzes lernen. Dadurch lassen sich ähnliche, täuschend echte Bilder erzeugen, die wiederum für ein anderes Netz verwendet werden können. Diese Technik verwendet man häufig bei Daten für die es sehr schwer ist gelabelte Daten zu bekommen, z.B. Sensordaten für Aktivitätserkennung.\n",
    "\n",
    "Da der Fokus hier auf dem Klassifizieren liegt, werden wir nicht weiter darauf eingehen. Bei Interesse dürfen Sie natürlich gerne versuchen Augmentierungen hinzuzufügen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Wir normalisieren die Bilder mit folgenden Werten. Beachte, wegen RGB drei Mittelwerte und drei Standardabweichungen.\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Wir entscheiden uns hier für eine Batchgröße von 16. Sie können gerne auch größere oder kleinere Batches ausprobieren.\n",
    "batch_size = 16\n",
    "\n",
    "# Eine einfache Funktion, die die Bilder läd, normalisiert und zu einem Pytorch-Tensor transformiert.\n",
    "def simple_dataloader(filename):\n",
    "    cur_img = Image.open(filename)\n",
    "    img_as_array = np.array(cur_img)\n",
    "    \n",
    "    data_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=means, std=stds),\n",
    "    ])\n",
    "\n",
    "    return data_transforms(img_as_array)\n",
    "\n",
    "\n",
    "# Sammele alle Eingabebilder zusammen mit den Labels (Ordnername)\n",
    "train_dataset = torchvision.datasets.ImageFolder(dataset_path+\"train/\", loader=simple_dataloader)\n",
    "val_dataset = torchvision.datasets.ImageFolder(dataset_path+\"val/\", loader=simple_dataloader)\n",
    "\n",
    "# Speichere die Anzahl an Trainings- und Validierungsdaten für die spätere Normierung des Losses.\n",
    "dataset_sizes = dict()\n",
    "dataset_sizes[\"train\"] = len(train_dataset)\n",
    "dataset_sizes[\"val\"] = len(val_dataset)\n",
    "\n",
    "# Ein Dataloader kann die geladenen Bilder mischen, gruppiert die Bilder zu entsprechenden Batches und lässt die \n",
    "# letzte (möglicherweise unvollständige) Batch auf Wunsch entfallen.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, drop_last=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, num_workers=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotten von Zwischenergebnissen\n",
    "Später, während des Trainings und der Validierung, möchten wir gerne einige Zwischenergebnisse sehen. Hierfür verwenden wir die nachfolgende Methode. Dabei wird in Ihrem Verzeichnis ein Ordner mit dem Namen *Some Results [YYYY-MM-DD HH_MM_SS]* (Zeitstempel kommt von oben) angelegt. Darin finden Sie nach ausreichend langem Training, zwei Unterordner *train* und *val*. Beide Unterordner enthalten die Zwischenergebnisse. Die Dateinamen sehen wie folgt aus:\n",
    "  \n",
    "<br>\n",
    "<center>Epoch_e;cls_i.png</center>\n",
    "\n",
    "und bedeuten, dass das gezeigte Bild in der Epoche e, Iteration i, als Klasse cls klassifiziert wurde.\n",
    "<br>  \n",
    "<br>  \n",
    "<br>  \n",
    "\n",
    "## Pytorch Tensoren und ihre Dimensionsreihenfolge\n",
    "Ein Pytorch Tensor hat in der Regel folgende shape:\n",
    "\n",
    "<br>\n",
    "<center>[batch_size, num_channels, height, width]</center>\n",
    "\n",
    "Das heißt, um ein RGB Bild zu plotten müssen wir uns erst einmal für einen Batchindex entscheiden. Um unnötigen Ärger mit nichtvorhandenen Indizes zu vermeiden, wählen wir im nächsten Abschnitt *batch_idx = 0*, somit bekommen wir auch keinen Fehler, selbst wenn wir mit einer Minibatch von *batch_size = 1* trainieren. Folglich ist *inp* von der Form:\n",
    "\n",
    "<br>\n",
    "<center>[num_channels, height, width]</center>\n",
    "\n",
    "Ein weiterer Unterschied bezüglich der Dimensionsreihenfolge zwischen der Pytorch Tensoren und numpy Arrays ist, dass bei numpy Arrays die Channels am Ende stehen. Um diese Reihenfolge anzupassen, verwenden wir im folgenden:\n",
    "\n",
    "<br>\n",
    "<center>inp.transpose((1, 2, 0))</center>\n",
    "\n",
    "Das heißt die erste Dimension wird an die nullte Dimension, die zeite Dimension an die erste Dimension und die nullte an die zweite Dimension verschoben. Oft spricht man auch von *channel first* oder *channel last*.\n",
    "\n",
    "<br>\n",
    "Da die Pytorch Tensoren Float-Tensoren sind, enthalten die Channels Werte zwischen 0 und 1. Wenn wir aber mit PIL (Pillow) ein Bild speichern möchten, müssen wir den Tensor zu einem uint8 Format casten (Restklassenring Z/255Z) und sicherstellen, dass die Werte zwischen 0 und 255 liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_some_results(inputs, preds, class_map, img_counter, epoch, date_time, phase):\n",
    "    batch_idx = 0\n",
    "    \n",
    "    # Den Teil \".cpu().detach()\" brauchen wir hier nicht unbedingt, da wir nicht auf einer Grafikkarte arbeiten.\n",
    "    # Könnte aber für alle interessant sein, die das Zuhause auf ihrem eigenen PC mit GPU Unterstüzung laufen lassen.\n",
    "    inp = np.array(inputs.cpu().detach().numpy()[batch_idx], dtype=np.float32)\n",
    "    \n",
    "    # Da die Bilder zuvor normalisiert wurden, müssen wir dies erst rückgängig machen.\n",
    "    for channel in range(3):\n",
    "        inp[channel] = inp[channel] * stds[channel] + means[channel]\n",
    "        inp[channel] = np.minimum(np.maximum(inp[channel], 0), 1)    \n",
    "    \n",
    "    inp = np.array(inp.transpose((1, 2, 0)) * 255.0, dtype=np.uint8)\n",
    "    class_name = class_map[int(preds[batch_idx])]\n",
    "\n",
    "    save_dir_results = os.getcwd() + \"/Some Results [\" + date_time + \"]/\" + phase + \"/\"\n",
    "\n",
    "    if not os.path.isdir(save_dir_results):\n",
    "        os.makedirs(save_dir_results)\n",
    "\n",
    "    pil_save_img = Image.fromarray(np.array(inp, dtype=np.uint8))\n",
    "    pil_save_img.save(save_dir_results + \"Epoch_\" + str(epoch) + \";\" + class_name + \"_\" + str(img_counter) + \".png\")\n",
    "    img_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein Convolutional Neural Network definieren\n",
    "Im folgenden verwenden wir ein bereits implementiertes CNN (Convolutional Neural Network). Sie können auch gerne andere Netze ausprobieren. [Hier](https://pytorch.org/docs/stable/torchvision/models.html) gibt es einen Überblick über bereits verfügbare Netze. Eine andere Möglichkeit wäre, dass Sie Ihr eigenes Netz implementieren, sehen Sie z.B. [hier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network). \n",
    "<br>  \n",
    "Damit unser Modell schneller zu einer akzeptablen Lösung konvergiert, verwenden wir vortrainierte Gewichte. Diese Technik nennt sich Transferlearning. Die Idee ist, dass man ein Netz auf einem anderen (meist sehr viel größeren) Datensatz trainiert und danach auf dem eigenen (meist kleinen) Datensatz anwendet. Dabei spielt es kaum eine Rolle ob die Datensätze ähnlich zueinander sind oder nicht, da die sehr einfachen Feature (z.B. Kanten, Ecken etc) bei vielen Datensätze ähnlich sind. Die Datensätze auf denen zuvor trainiert wurde, haben meist nicht die gleiche Anzahl an Klassen, wie wir sie benötigen. Daher schneiden wir den letzten Layer ab und setzen einen neuen Layer mit der von uns benötigten Anzahl an Klassen bzw Neuronen dran. Der letzte Layer ist bei Klassifizierern in der Regel ein Fully-Connected Layer. Die neu zugewiesene Gewichte kennen wir natürlich nicht und wählen daher eine zufällige Belegung.\n",
    "<br>  \n",
    "Im folgenden verwenden wir ResNet34. Dieses Netz enthält 34 Layer. Aus der ResNet Reihe gibt es auch noch ResNet18, ResNet50, ResNet101 und ResNet152. In der Tat hat das ResNet152 exakt 152 Layer und gehört damit zu den tiefsten CNNs. Je tiefer die Netze, desto schwieriger ist es diese zu trainieren. Einerseits akkumulieren sich Fehler sehr schnell zu sehr großen Werten, was wiederum zu sehr großen Gradienten führen kann (auch bekannt als exploding gradients bekannt). Andererseits ist das Training sehr langsam und benötigt darüberhinaus sehr viel Speicher. Wir haben uns für ResNet34 entschieden, da es ein guter Kompromiss zwischen Geschwindigkeit und Genauigkeit ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet34(pretrained, num_classes=1000):\n",
    "    model = torchvision.models.resnet34(pretrained=pretrained)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementieren der Trainingsmethode (ToDo)\n",
    "Im folgenden sollen Sie die Methode zum Trainieren des Klassifizierers vervollständigen. Sie können sich an der Trainingsmethode zur Regression orientieren und auch in den folgenden Tutorials Hinweise finden:\n",
    "1. [Ein Tutorial über Transferlearning, am Beispiel von Bienen und Ameisen](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#training-the-model)\n",
    "2. [Training eines Klassifizierers, am Beispiel von Cifar10](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-bd24d370868d>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-bd24d370868d>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    init_lr =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import sys\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def train_model(num_epochs=100):\n",
    "    since = time.time()\n",
    "    \n",
    "    model = resnet34(pretrained=True, num_classes=len(class_map_idx_to_str))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Definieren Sie hier das Loss\n",
    "    \n",
    "    # Setzen Sie eine Lernrate\n",
    "    init_lr = \n",
    "    \n",
    "    # Wählen Sie einen Optimierer\n",
    "    optimizer = torch.optim.\n",
    "    \n",
    "    # Optional können Sie einen Scheduler für die Lernrate festlegen.\n",
    "    scheduler = lr_scheduler.\n",
    "    \n",
    "    # Setzen Sie ein Loss, das definitiv größer ist als das erste Validierungsloss.\n",
    "    last_loss = \n",
    "    \n",
    "    # Für den Fall, dass wir keinen Checkpoint finden, müssen wir die Startepoche festlegen\n",
    "    epoch = 0\n",
    "    \n",
    "    # Wir versuchen einen Checkpoint zu laden\n",
    "    print(\"checkpoint_load_path =\", checkpoint_load_path)    \n",
    "    epoch, last_loss = create_or_load_checkpoint(model, epoch, optimizer, last_loss)\n",
    "    \n",
    "\n",
    "    # Implementieren Sie das Training und die Validierung\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best loss Acc: {:4f}'.format(last_loss))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluieren des trainierten Modells (ToDo)\n",
    "Vervollständigen Sie die Methode test_classifier() so, dass eine Confusion Matrix für alle Testbilder berechnet und visualsiert wird. In *all_imgs* sind alle Pfade zu den Testbildern zu finden.  \n",
    "<br>\n",
    "Hinweise: \n",
    "1. Verwenden Sie die Funktion *simple_dataloader(img_path)* aus der zweiten Zelle (siehe oben), um ein Bild zu laden und einen normalisierten Float-Tensor zurück zu bekommen.\n",
    "2. Nach Anwenung der Funktion *simple_dataloader* enthalten Ihre Daten noch keine Batches. Verwenden Sie hierzu, ähnlich wie in Aufgabe 6.2 die Methode unsqueeze_(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-3051925be2f3>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3051925be2f3>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    conf_matrix =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_model():\n",
    "    inverted_cls_dict = {value: key for key, value in class_map_idx_to_str.items()}\n",
    "    num_classes = len(class_map_idx_to_str)\n",
    "    print(\"num_classes =\", num_classes)\n",
    "    print(\"class_map_idx_to_str =\", class_map_idx_to_str)\n",
    "    print(\"inverted_cls_dict =\", inverted_cls_dict)\n",
    "\n",
    "    all_imgs = [path for path in glob.glob(dataset_path + \"test\" + os.sep + \"*\" + os.sep + \"*.png\")]\n",
    "\n",
    "    model = resnet34(pretrained=False, num_classes=len(class_map_idx_to_str))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Lade den Checkpoint\n",
    "    print(\"checkpoint_load_path =\", checkpoint_load_path)\n",
    "    epoch, last_loss = create_or_load_checkpoint(model)\n",
    "\n",
    "    # Manche Modelle besitzen Dropout oder eine Batchnorm. Diese verhalten sich im Trainingsmodus anders als im Testmodus.\n",
    "    # Mit model.eval() legen wir fest, dass wir im Testmodus sind.\n",
    "    model.eval()    \n",
    "    \n",
    "    # Initialisiere ein Array für die Confusion Matrix. Welche Größe hat diese?\n",
    "    conf_matrix = \n",
    "\n",
    "    # Itereiere über jedes Element der Liste all_imgs. Lade das korrespondierende Bild und übergib es dem Modell.\n",
    "    # Woher kennen Sie das Label des Bildes? Nutzen Sie das Dictionary inverted_cls_dict um von der tatsächlichen\n",
    "    # Klasse zur Klassen-ID zu schließen.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nConfusion Matrix =\\n\", conf_matrix)\n",
    "    \n",
    "    # Visualisieren Sie die Confusion Matrix mit matplotlib\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training und Valdierung starten\n",
    "Wählen Sie zunächste den Modus *train* aus. Wenn *train = True* trainieren Sie das gewählte Modell. Ein Training mit 10 Epochen und ResNet34 sollte etwa 30 Minuten dauern unter großer Auslastung evtl etwas länger. Wenn Sie ein vortrainiertes Netz und passende Hyperparameter wählen, können Sie bereits nach einer oder zwei Epochen zufriedenstellende Ergebnisse erreichen. Die Testphase sollte sehr viel schneller sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5bf25c9a660a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_map_idx_to_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train = True \n",
    "\n",
    "\n",
    "if train:\n",
    "    model_ft = train_model()\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
