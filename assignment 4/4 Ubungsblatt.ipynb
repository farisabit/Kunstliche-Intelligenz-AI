{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.+Exercise+Sheet+Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0_D_GNi1rVc"
      },
      "source": [
        "# 4. Übungsblatt\r\n",
        "\r\n",
        "by \r\n",
        "- Javier Carrasco Melo\r\n",
        "- Fariza Sabit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmB62gpvVd7f"
      },
      "source": [
        "# Aufgabe 4.3\n",
        "Zunächst definieren wir die Pfade zum Lesen der Roddaten und Schreiben die Feature-Matrix. Falls der Ordner \"Feature_Matrix\" noch nicht in Ihrem aktuellen Verzeichnis existiert, wird der Ordner angelegt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0L7gOMPVd7u"
      },
      "source": [
        "import os\n",
        "reduced_train_dataset_path = \"/home/share/AR/AR_Train.dat\"\n",
        "reduced_test_dataset_path = \"/home/share/AR/AR_Test.dat\"\n",
        "\n",
        "feature_matrix_dir = os.getcwd() + os.sep + \"Feature_Matrix\" + os.sep\n",
        "train_feature_matrix_path = feature_matrix_dir + \"Train_Feature_Matrix.dat\"\n",
        "test_feature_matrix_path = feature_matrix_dir + \"Test_Feature_Matrix.dat\"\n",
        "\n",
        "# Wenn der Ordner Feature_Matrix noch nicht existiert legen wir ihn im aktuellen Verzeichnis an.\n",
        "if not os.path.isdir(feature_matrix_dir):\n",
        "    os.makedirs(feature_matrix_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt1SurcXVd7x"
      },
      "source": [
        "Als nächstes definieren wir ein Dictionary, das von der Class-ID zum entsprechenden Namen abbildet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgyxUGoeVd7y"
      },
      "source": [
        "class_map_to_string = {0.0: \"Null\",\n",
        "                       1.0: \"Stand\",\n",
        "                       2.0: \"Walk\",\n",
        "                       4.0: \"Sit\",\n",
        "                       5.0: \"Lie\",\n",
        "                      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP3wh15vVd7z"
      },
      "source": [
        "In der folgenden Funktion wird die Feature-Matrix berechnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T-DfWp6Vd70"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def create_feature_matrix():\n",
        "    reduced_train_dataset = None\n",
        "    reduced_test_dataset = None\n",
        "\n",
        "    train_feature_matrix = None\n",
        "    test_feature_matrix = None\n",
        "\n",
        "    phases = [\"train\", \"test\"]\n",
        "    for phase in phases:\n",
        "\n",
        "        if phase == \"train\":\n",
        "            all_data = np.loadtxt(reduced_train_dataset_path)\n",
        "        else:\n",
        "            all_data = np.loadtxt(reduced_test_dataset_path)\n",
        "\n",
        "\n",
        "        timestamps = all_data[:, 0]\n",
        "        labels = all_data[:, -1]\n",
        "\n",
        "        sensor_data = all_data[:, [[1,2,3]]].astype(float)\n",
        "        \n",
        "        reverse_time = list(timestamps).copy()\n",
        "        reverse_time.reverse()\n",
        "\n",
        "        mean_sampling_rate = np.array([reverse_time[i] - reverse_time[i + 1] for i in range(len(reverse_time) - 1)]).mean()\n",
        "        num_samples_per_sec = round(1000.0 / float(mean_sampling_rate), 2)\n",
        "\n",
        "        window_size_in_sec = 3.0\n",
        "        num_elements_in_window = int(window_size_in_sec * num_samples_per_sec)\n",
        "        window_shift_size = int(float(num_elements_in_window) / 4.0)\n",
        "        \n",
        "        \"\"\"\n",
        "        Jede Zeile der Feature-Matrix correspondiert zu einem Fenster F_i. Die letzte Spalte einer Zeile i in der \n",
        "        Feature-Matrix enthält das Label L_i, alle anderen Spalten enthalten die Feature-Werte des entsprechenden\n",
        "        Fensters.\n",
        "        \"\"\"\n",
        "        temp_feature_matrix = []\n",
        "\n",
        "        start = 0\n",
        "        end = num_elements_in_window\n",
        "\n",
        "        while end < len(labels):\n",
        "            print_string = \"Aktuelle Endposition des Fensters: \" + str(end) + \"/\" + str(len(labels))\n",
        "            sys.stdout.write('\\r'+print_string)\n",
        "            sys.stdout.flush()\n",
        "            next_labels = list(labels[start:end])\n",
        "            \n",
        "            \"\"\"\n",
        "            class_count_dict ist ein Dictionary, wobei die Keys die Klassen-IDs speichern und die Values,\n",
        "            die Häufigkeit einer Klasse innerhalb des aktuellen Fensters angibt.\n",
        "            \"\"\"\n",
        "            class_count_dict = Counter(next_labels)\n",
        "\n",
        "\n",
        "            num_classes = len(class_count_dict.values())\n",
        "\n",
        "            \"\"\"\n",
        "            argmax_cls ist der Key in class_count_dict, der den maximalen Wert hat. Folglich enthält: \n",
        "                class_count_dict[argmax_cls]\n",
        "            die Anzahl der am meisten vorkommenden Labels (Majority Vote).\n",
        "            \"\"\"\n",
        "            argmax_cls = max(Counter(next_labels), key=Counter(next_labels).get)\n",
        "\n",
        "            next_window = sensor_data[start:end, :]\n",
        "\n",
        "            \"\"\"\n",
        "            Statt nur nach dem majority vote zu gehen verwenden wir noch eine weitere Bedingung. Die Anzahl der Labels innerhalb\n",
        "            eines Fensters muss mindestens 75% aller Labels des aktuellen Fensters entsprechen. Wenn das nicht zutrifft, oder\n",
        "            es einen Fehler im Sensor gab (NaN), dann verwerfen wir das aktuelle Fenster und berechnen auch keine Feature für\n",
        "            dieses Fenster.\n",
        "            \"\"\"\n",
        "            if float(class_count_dict[argmax_cls]) / float(num_elements_in_window) > 0.75 and not np.isnan(np.sum(next_window)):\n",
        "                temp_feature_matrix.append(compute_features(next_window, mean_sampling_rate) + [argmax_cls])\n",
        "\n",
        "            start = start + window_shift_size\n",
        "            end = end + window_shift_size\n",
        "\n",
        "        if len(temp_feature_matrix) > 0:\n",
        "            temp_feature_matrix = np.array(temp_feature_matrix).reshape(len(temp_feature_matrix), len(temp_feature_matrix[0]))\n",
        "\n",
        "            if phase == \"train\":\n",
        "                if train_feature_matrix is None:\n",
        "                    train_feature_matrix = temp_feature_matrix.copy()\n",
        "                else:\n",
        "                    train_feature_matrix = np.vstack((train_feature_matrix, temp_feature_matrix))\n",
        "            else:\n",
        "                if test_feature_matrix is None:\n",
        "                    test_feature_matrix = temp_feature_matrix.copy()\n",
        "                else:\n",
        "                    test_feature_matrix = np.vstack((test_feature_matrix, temp_feature_matrix))\n",
        "\n",
        "    np.savetxt(train_feature_matrix_path, train_feature_matrix, '%1.9f')\n",
        "    np.savetxt(test_feature_matrix_path, test_feature_matrix, '%1.9f')\n",
        "\n",
        "    return train_feature_matrix, test_feature_matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXKrd_ncVd73"
      },
      "source": [
        "## Berechne Feature (ToDo siehe 8. Woche)\n",
        "Die folgende Funktion berechnet bisher die zwei sehr einfachen Feature min und max. Überlegen Sie sich weitere Feature (z.B. Durchschnitt, Standardabweichung etc., insgesamt mindestens fünf) und implementieren Sie diese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFtVNmJnVd74"
      },
      "source": [
        "def compute_features(window_values, mean_sampling_rate):\n",
        "    features = []\n",
        "    \n",
        "    for i in range(len(window_values[0])):\n",
        "        cur_sensor_data = window_values[:,i]\n",
        "\n",
        "        x_values = [elem[0] for elem in cur_sensor_data]\n",
        "        y_values = [elem[1] for elem in cur_sensor_data]\n",
        "        z_values = [elem[2] for elem in cur_sensor_data]\n",
        "        magnitudes = [np.sqrt(elem[0] ** 2 + elem[1] ** 2 + elem[2] ** 2) for elem in cur_sensor_data]\n",
        "\n",
        "        num_samples_in_window = float(len(magnitudes))\n",
        "\n",
        "        for liste in [x_values, y_values, z_values, magnitudes]:\n",
        "            # Definiere hier einige Feature\n",
        "            min_value = min(liste)\n",
        "            max_value = max(liste)\n",
        "            sd = np.std(liste)\n",
        "            mean = np.mean(liste)\n",
        "            median = np.median(liste)\n",
        "            iqr = np.subtract(*np.percentile(liste, [75, 25]))\n",
        "\n",
        "            features.extend([min_value, max_value, sd, mean, median, iqr])\n",
        "           \n",
        "\n",
        "            # Füge die neu definierten Feature zu dieser Liste hinzu:\n",
        "            features.extend([min_value, max_value])  # ToDo\n",
        "            \n",
        "\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuP8O_nlVd75"
      },
      "source": [
        "## Normalisieren (ToDo siehe 9. Woche)\n",
        "Als nächstes sollen Sie die Daten normalisieren. Sie können entweder die Klassen StandardScaler oder MinMaxScaler von sklearn verwenden, oder die Normalisierung selbst implemnetieren. Achten Sie beim Normalisieren darauf, dass sie die entsprechenden Konstanten (beim StandardScaler z.B. Mittelwert und Standardabweichung) nur auf den Trainingsdaten berechnen und die gleichen Konstanten beim Normalisieren des Testdatensatzes verwenden. Wieso machen wir das so? Bei den oben genannten Klassen, gibt es die Methode fit_transform(), diese berechnet die Konstanten und die Methode transform(), normalisiert die Daten gemäß der zu vor gespeicherten Konstanten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnfbvPzbVd75"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def normalize_data(x_train_unnormalized, x_test_unnormalized):    \n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train_unnormalized)\n",
        "    x_train_normalized = scaler.transform(x_train_unnormalized)\n",
        "    x_test_normalized = scaler.transform(x_test_unnormalized)\n",
        "    \n",
        "    return x_train_normalized, x_test_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55E3Oq2TVd76"
      },
      "source": [
        "## Klassifizieren (ToDo)\n",
        "Vervollständigen Sie die Methode classify so, dass die folgenden Klassifizierer verwendet werden können:\n",
        "- k-NN\n",
        "- Decision Tree\n",
        "- Support Vector Machine (in sklearn heißt die Regression Version SVM und die Klassifizierer Version SVC)\n",
        "- Multilayer-Perzeptron\n",
        "\n",
        "und das MLP einen $F_1$-Score von $80\\%$ oder besser auf dem Testdatensatz erreicht. Wie üblich soll der Klassifizierer mit dem Trainingssatz train_data trainiert und anschließend auf dem Testdatensatz test_data getestet werden. Nach dem Test soll eine Evaluation des Testergebnisses durchgeführt werden. Dabei soll der $F_1-$Score und die Confusion-Matrix berechnet werden. Abschließend soll die Confusion Matrix als Heatmap geplottet werden (vergleiche 9. Woche)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jInLgXY7Vd77"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import NuSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "\n",
        "def classify(train_data, test_data, cls_name=None):\n",
        "    x_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
        "    x_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "    # DO NOT CHANGE -->\n",
        "    # Entferne alle Zeilen der Feature-Matrix mit Klassen-ID 0 (Null-Class, siehe Zusatzmaterial)\n",
        "    all_null_indices_train = [i for i, lbl in enumerate(y_train) if lbl == 0]\n",
        "    all_null_indices_test = [i for i, lbl in enumerate(y_test) if lbl == 0]\n",
        "    x_train = [elem for i, elem in enumerate(x_train) if not i in all_null_indices_train]\n",
        "    y_train = [elem for i, elem in enumerate(y_train) if not i in all_null_indices_train]\n",
        "    x_test = [elem for i, elem in enumerate(x_test) if not i in all_null_indices_test]\n",
        "    y_test = [elem for i, elem in enumerate(y_test) if not i in all_null_indices_test]\n",
        "    # <--\n",
        "\n",
        "    # Normalisiere x_train und x_test\n",
        "    x_train_normalized, x_test_normalized = normalize_data(x_train, x_test)\n",
        "    \n",
        "    # Implementiere die Klassifizierung\n",
        "    # ToDo:\n",
        "    \n",
        "    if cls_name:\n",
        "        cls_names = [cls_name]\n",
        "    else: \n",
        "        cls_names = [\"Decision Tree\", \"Support Vector Machine\", \"GaussianNB\", \"BernoulliNB\", \n",
        "                     \"RandomForestClassifier\", \"AdaBoostClassifier\", \"Multilayer-Perzeptron\", \"k-NN\"]\n",
        "    \n",
        "    for cls_name in cls_names:\n",
        "        if cls_name == \"k-NN\":\n",
        "            cls = KNeighborsClassifier()   # n_neighbors=5\n",
        "        elif cls_name == \"Decision Tree\":\n",
        "            cls = DecisionTreeClassifier()\n",
        "        elif cls_name == \"Support Vector Machine\":\n",
        "            cls = SVC()\n",
        "        elif cls_name == \"GaussianNB\":\n",
        "            cls = GaussianNB()\n",
        "        elif cls_name == \"BernoulliNB\":\n",
        "            cls = BernoulliNB()\n",
        "        elif cls_name == \"RandomForestClassifier\":\n",
        "            cls = RandomForestClassifier()\n",
        "        elif cls_name == \"AdaBoostClassifier\":\n",
        "            cls = AdaBoostClassifier()\n",
        "        elif cls_name == \"Multilayer-Perzeptron\":\n",
        "            cls = MLPClassifier(hidden_layer_sizes=111, max_iter=200)\n",
        "        \n",
        "        cls.fit(x_train, y_train)\n",
        "        y_pred = cls.predict(x_test)\n",
        "        f1 = round(f1_score(y_test, y_pred, average=\"micro\"), 3)\n",
        "        print(f\"{cls_name}'s F_1: {f1}\")\n",
        "    \n",
        "    return cls, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGIPyglKVd7_"
      },
      "source": [
        "Falls die Feature Matrizen \"Train_Feature_Matrix.dat\" und \"Test_Feature_Matrix.dat\" schon existieren, wird direkt die classify Methode ausgeführt, andernfalls werden vorher noch die Feature-Matrizen berechnet. Achten Sie also darauf in der Methode compute_features die Feautureberechnung anzupassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmh_ZPdiVd8A"
      },
      "source": [
        "if not (os.path.isfile(train_feature_matrix_path) and os.path.isfile(test_feature_matrix_path)):\n",
        "    train_feature_matrix, test_feature_matrix = create_feature_matrix()\n",
        "else:\n",
        "    train_feature_matrix =  np.loadtxt(train_feature_matrix_path)\n",
        "    test_feature_matrix = np.loadtxt(test_feature_matrix_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X02IybLVVd8B",
        "outputId": "5a9a952f-1ce7-4438-89e9-0d26a5054c2f"
      },
      "source": [
        "# protip: if you run it multiple times, you'll eventually get an MLP with F_1 > 0.8\n",
        "cls, x_test, y_test = classify(train_feature_matrix, test_feature_matrix) #, cls_name=\"k-NN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree's F_1: 0.801\n",
            "Support Vector Machine's F_1: 0.755\n",
            "GaussianNB's F_1: 0.814\n",
            "BernoulliNB's F_1: 0.708\n",
            "RandomForestClassifier's F_1: 0.805\n",
            "AdaBoostClassifier's F_1: 0.78\n",
            "Multilayer-Perzeptron's F_1: 0.817\n",
            "k-NN's F_1: 0.822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqbOM1gjVd8E"
      },
      "source": [
        "Unless otherwise is stated, the confusion matrix (following blocks) is based on the model trained using k-NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ohsagd3Vd8E",
        "outputId": "f9dbece5-2e0a-4d79-c380-4e3b4bf439e6"
      },
      "source": [
        "# this matrix shows in the diagonal the correctly identified classes (e.g., stehen, sitzen, liegen...)\n",
        "conf_matrix = confusion_matrix(y_test, cls.predict(x_test))\n",
        "conf_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[287,  11,  37,   0],\n",
              "       [ 44,  91,   0,   0],\n",
              "       [ 55,   0, 247,   0],\n",
              "       [  0,   0,   0,  52]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO3ifZpCVd8F",
        "outputId": "b03f5d4d-4d4f-487f-e0e5-4a580a021851"
      },
      "source": [
        "plot_confusion_matrix(cls, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f835a475860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcI-HzC8Vd8G"
      },
      "source": [
        "# Aufgabe 4.4\n",
        "\n",
        "Ein binärer Klassifizierer $K_1$ liefert auf den Validierungsdaten die nachfolgende Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2AG9cc1Vd8G"
      },
      "source": [
        "# we will asume that Klasse A will represent a positive outcome, and that Klasse B will represent a negative outcomme\n",
        "TP = 100\n",
        "TN = 5\n",
        "FP = 8\n",
        "FN = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PMtgFvlVd8G"
      },
      "source": [
        "Beantworten Sie die folgenden Fragen:\n",
        "\n",
        "1. **Wie viele Daten gehören zur Klasse A und wie viele zur Klasse B?**\n",
        "\n",
        "*Klasse A*: 102 elements; *Klasse B*: 13 elements\n",
        "\n",
        "2. **Berechnen Sie Precision, Recall, Accuracy und den F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuXEdsyTVd8G",
        "outputId": "ff530ff6-6797-4480-a2e4-b7f3c15ccaba"
      },
      "source": [
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "f1 = 2 * TP / (2 * TP + FP + FN)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F_1: {f1}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.9259259259259259\n",
            "Recall: 0.9803921568627451\n",
            "Accuracy: 0.9130434782608695\n",
            "F_1: 0.9523809523809523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoCLm8K4Vd8H"
      },
      "source": [
        "3. **Ist eine dieser Messungen ausreichend um die Qualität des Klassifizierers zu beschreiben? Wenn ja, welche Messung beschreibt die Qualität des Klassifizierers am besten. Falls nicht was ist das Problem?**\n",
        "\n",
        "$F_1$ is better than the Accuracy when we have imbalanced data distribution, like in this exercise. Klasse A has much more elements than Klasse B, about 90% to 10%.\n",
        "\n",
        "4. **Angenommen wir haben einen zweiten Klassifizierer K2 der über eine Gleichverteilung zufällig entscheidet ob ein Sample zur Klasse A oder B gehört. Welche Accuracy hat K2?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFYSYxQbVd8H",
        "outputId": "f2f43207-c490-4fae-f4f8-c3917a3ac457"
      },
      "source": [
        "# we will use new values to calculate our accuracy\n",
        "TP = 51\n",
        "TN = 6\n",
        "FP = 7\n",
        "FN = 51\n",
        "\n",
        "accuracy_2 = (TP + TN) / (TP + TN + FP + FN)\n",
        "f1_2 = 2 * TP / (2 * TP + FP + FN)\n",
        "\n",
        "print(f\"Accuracy 2: {accuracy_2}; accuracy 1: {accuracy}\")\n",
        "print(f\"F_1 2: {f1_2}; F_1 1: {f1}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 2: 0.4956521739130435; accuracy 1: 0.9130434782608695\n",
            "F_1 2: 0.6375; F_1 1: 0.9523809523809523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evQejEyKVd8H"
      },
      "source": [
        "5. **Welcher Klassifizierer ist für die korrekte Klassifizierung von Samples aus der Klasse B besser, K1 oder K2?**\n",
        "\n",
        "K2 will do a slightly better job at correctly classifying (i.e., recognizing true negatives) samples from Klasse B: 6 or 7 vs 5 (K1).\n",
        "\n",
        "6. **Was würden Sie empfehlen, um K1 zu verbessern?**\n",
        "\n",
        "During the data collection, if it is possible, we would recommend to make the samples more balanced in terms of Klasse A and Klasse B, getting more data from Klasse A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmtHVf2Vd8I"
      },
      "source": [
        "# Aufgabe 4.5\n",
        "\n",
        "Erklären Sie die folgenden Klassifizierer in Ihren eigenen Worten (je ca 5-10 Sätze) mit Abbildungen und ggf. Berechnungsformeln:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLNSWe3QVd8I"
      },
      "source": [
        "## (a) k-Nearest Neighbor\n",
        "\n",
        "KNN is a simple and powerful tool used for both classification and regression in supervised learning. It can be a benchmark for other more complicated neural-network-based classifiers because\n",
        "*  KNN is a *non-parametric*, meaning that the algorithm is free to learn any functional form from the training data. Thus the probabilty of mismodeling the underlying function is low; \n",
        "\n",
        "* KNN is instance-based making the algorithm to adapt to new data easily.\n",
        " \n",
        "However, there are main disadvantages of instance-based learning algorithm:\n",
        "\n",
        "* Fast training, slow testing (Each query calculates the local model from scratch);\n",
        "* Memory inefficient;\n",
        "* A risk of overfitting due to remembering all training instances.\n",
        "\n",
        "**Idea:** In the 'training' phase the algorithm stores the feature vectors and labels of the training dataset. In the testing phase, the algorithm finds the k nearest neighbours by comparing the Euclidean (but not necessary) distance:\n",
        "\n",
        "$d(x_i, x)= \\sqrt{(x_1-x)^2+(x_2-x)^2+...+(x_n-x)^2}$\n",
        "\n",
        "Then looks up labels of those k closest neighbors and\n",
        "assigns majority vote label for the input $x$:\n",
        "$P(y=j | X=x ) = \\frac{1}{K}\\sum I(y=j)$\n",
        "\n",
        "<img src=\"https://kevinzakka.github.io/assets/knn/20nn.png\" width=\"400\" height=\"400\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmvwhTKqVd8I"
      },
      "source": [
        "## (b) Decision Tree\n",
        " \n",
        "Decision Tree algorithm is an intuitive, non-parametric supervised learning technique used to solve both classification and regression problems. Here the algorithm learns decision rules by continuiosly splitting the data according to a certain parameter.  \n",
        "\n",
        "**Idea:** Classify an input by asking a series of carefully crafted questions about the attributes of the dataset until we come up with the class label. The series of questions and their possible answers will be organized in the form of a hierarchical decision tree consisting of nodes (input), edges (attributes) and lead nodes (class label). \n",
        "\n",
        "One of the algorithms used in Decision Trees is Classification And Regression Tree (CART). CART uses Gini Impurity score to optimally split the dataset into a decision tree (the lower the better) [[1](https://books.google.de/books/about/Hands_On_Machine_Learning_with_Scikit_Le.html?id=HnetDwAAQBAJ&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false)] :\n",
        "\n",
        "$$G_i = 1 - \\sum ^n_{k=1} p_{i,k}^2 $$ \n",
        "where $p_{i,k}$ is the estimated probaility of class k in the ith node.\n",
        "\n",
        "*Remark:* Instead of Gini impurity one can also use Entropy impurity measure.\n",
        "\n",
        "Some drawbacks:  \n",
        "- Unstable, i.e. a small change in the data can lead to a large change in the structure of the tree;\n",
        "- Relatively expensive as the comptational complexity and time has taken are more.\n",
        "\n",
        "<img src=\"https://i1.wp.com/dataaspirant.com/wp-content/uploads/2017/01/B03905_05_01-compressor.png?resize=768%2C424&ssl=1\" width = 500 heght=400>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fsC13PjVd8I"
      },
      "source": [
        "## (c) Support Vector Machine (SVM)\n",
        "\n",
        "SVM is a supervised learning model, particularly well suited for classification of complex but small- or medium-sized datasets [[3](https://books.google.de/books/about/Hands_On_Machine_Learning_with_Scikit_Le.html?id=HnetDwAAQBAJ&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false)].\n",
        "\n",
        "**Idea:** The algorithm finds the boundary with maximum margin between classes. The equation for a descision hyperplane $\\mathbb{R}^D$ is $$w^Tx - \\beta=0$$\n",
        "Scale $w$ such that points $x_0$ closest to hyperplane $w$ have distance 1:\n",
        "$$\\alpha w^Tx - \\beta=0$$\n",
        "$$\\alpha = \\frac{|w^Tx - \\beta|}{\\parallel w \\parallel _2 }$$\n",
        "where $\\alpha w$ is called canonical hyperplane. Notice that for canonical hyperplanes that correctly classify all data $$y_i ( w^Tx_i - \\beta) \\geq 1$$\n",
        "\n",
        "An optimization problem then for SVMs is to find the normal vector $w$ to the canonical hyperplane with the smallest norm [[2](https://link.springer.com/article/10.1007/BF00994018)]:\n",
        "$$ \\min _w \\frac{1}{2} \\parallel w \\parallel _2 $$\n",
        "$$\\text{subject to  } y_i ( w^Tx_i - \\beta) \\geq 1$$\n",
        "\n",
        "The data points (*vectors*) $x$ closest to the margin define (*support*) the hyperplane $w$ - hence **support-vectors**.\n",
        "\n",
        "For non-separable data sets (not linear) we can relax the original SVM objective to allow for an error of $\\varepsilon_i$ for the $i$th data point by introducing slack variables: \n",
        "$$y_i ( w^Tx_i - \\beta) \\geq 1 - \\varepsilon _i$$\n",
        "\n",
        "Non-Linear SVMs use the so called \"Kernel trick\", which allow them to project input data into a higher-dimension space as depicted in the figure below.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Muhammad_Awais_Bin_Altaf/publication/272520997/figure/fig2/AS:601593388998663@1520442449352/Motivation-behind-non-linear-SVM-classifier.png\" weight=400  height=400>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5USENUuJVd8J"
      },
      "source": [
        "## (d) Multilayer Perceptron (MLP)\n",
        "\n",
        "MLP is a type of feed-forward artificial neural networks, made of multiple fully-connected perceptron layers. Here is an example of a very basic MLP:\n",
        "\n",
        "<img src=\"https://www.tutorialspoint.com/tensorflow/images/multi_layer_perceptron.jpg\" height=300 weight=300 >\n",
        "\n",
        "An MLP consists of one input layer, one or more hidden layers and one output layer. Each layer is made up by nodes that are connected to all nodes in the previous (except input) and the following layer (except output), i.e., it is fully connected.\n",
        "\n",
        "Each node (or neuron) calculates the weighted sum of all the connections from the previous layer and applies the value to an activation function, usually a sigmoid function. This process is done to each layer in one direction (this makes it feed-forward), from the input to the output layer, passing through the hidden layer.\n",
        "\n",
        "The training of the model is done through backpropagation. The output is compared to the expected result, getting the error. Then the algorithm calculates the contribution of each neuron to this error, going one layer at a time in reverse from the output to the input layer. This way, the algorithm finds the gradients that will be used to slightly tweak the weights of this network to reduce the error.\n",
        "\n",
        "Source: [The Nature of Code](https://natureofcode.com/book/chapter-10-neural-networks/)\n",
        "\n",
        "Some disadvantages of MLP:\n",
        "\n",
        "- fully connected layers $\\to$ no convolution layers $\\to$ too many parameters $\\to$ redunancy & inefficiency \n",
        "- vanishing gradient problem due to exponentially decreasing/increasing errors \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lQwR84Mo4o3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}